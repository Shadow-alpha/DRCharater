{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Appearance', 'Personality', 'History', \"Ilse's Notebook\", 'Wall Sealing arc', 'The Female Titan arc', 'Clash of the Titans arc', 'Royal Government arc', 'Return to Shiganshina arc', 'Marley arc', 'War for Paradis arc', 'Bad Boy', 'Vertical Maneuvering Equipment', 'Strength', 'Intelligence', 'Awakened power', 'Sword-Wielding Technique', 'Thunder Spears'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('character.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data['Levi Ackerman']['content'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "\n",
    "html = '''<div class=\"pi-data-value pi-font\">hello<br></div>'''\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello <class 'bs4.element.NavigableString'> None\n",
      "<br/> <class 'bs4.element.Tag'> br\n"
     ]
    }
   ],
   "source": [
    "value_el = soup.select_one(\".pi-data-value\")\n",
    "for ref in value_el.select(\".reference\"):\n",
    "    ref.decompose()  # 删除脚注引用\n",
    "text_lines = []\n",
    "for elem in value_el.children:\n",
    "    print(elem, type(elem), elem.name)\n",
    "    if elem.name == \"ul\":\n",
    "        ul_text = \"\\n\".join(li.get_text(\" \", strip=True) for li in elem.find_all(\"li\", recursive=False))\n",
    "        text_lines.append(ul_text)\n",
    "    elif elem.name:\n",
    "        text_lines.append(elem.get_text(\" \", strip=True))\n",
    "    elif isinstance(elem, str):\n",
    "        text_lines.append(elem.strip())\n",
    "val = \"\\n\".join(filter(None, text_lines))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analyse valid gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1082 0 220\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "path = \"./gt/character_fandom.json\"\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "def split_name_francise(s: str):\n",
    "    name = s.split('(')[0].strip()\n",
    "    francise = s.split('(', maxsplit=1)[-1][:-1].strip()\n",
    "    return name, francise\n",
    "\n",
    "def save_json(data, path):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "null_dict, invalid_dict = dict(), dict()\n",
    "fran_counter = defaultdict(int)\n",
    "incomplete_dict = dict()\n",
    "for k, item in data.items():\n",
    "    name, francise = split_name_francise(k)\n",
    "    if 'name' not in item:\n",
    "        null_dict[k] = item\n",
    "        fran_counter[francise] += 1\n",
    "        continue\n",
    "    if not item['infobox']:\n",
    "        incomplete_dict[k] = item\n",
    "\n",
    "    if name not in item['name']:\n",
    "        invalid_dict[k] = item\n",
    "        # fran_counter[francise] += 1\n",
    "\n",
    "\n",
    "# save_json(null_dict, 'character_null.json')\n",
    "save_json(invalid_dict, 'character_invalid.json')\n",
    "\n",
    "print(len(data), len(null_dict), len(invalid_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(incomplete_dict, \"incomplete.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1107 119 295\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "## old\n",
    "print(len(data), len(null_dict), len(invalid_dict))\n",
    "print(len(fran_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz, process\n",
    "\n",
    "white = load_json('re_search.json')\n",
    "will_be_removed = dict()\n",
    "for char, info in invalid_dict.items():\n",
    "    if char in white:\n",
    "        continue\n",
    "    name, fran = split_name_francise(char)\n",
    "    name_url = info['url'].split('wiki/')[-1]\n",
    "    score1 = fuzz.token_sort_ratio(name, info['name'])\n",
    "    score2 = fuzz.token_sort_ratio(name, name_url.replace('_', ' '))\n",
    "    if score1 < 80 and score2 < 80:\n",
    "        will_be_removed[char] = info\n",
    "        # will_be_removed[char] = {'name':info['name'], 'url': info['url']}\n",
    "        # will_be_removed[char] = info\n",
    "        # print((score1, score2, char, info['name'], info['url']))\n",
    "# save_json(will_be_removed, 'to_be___removed.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(will_be_removed)\n",
    "save_json(will_be_removed, 'temp.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "653"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1107-119-335"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "397"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fran_counter2 = defaultdict(int) \n",
    "for char in data.keys():\n",
    "    name, fran = split_name_francise(char)\n",
    "    fran_counter2[fran] += 1\n",
    "len(fran_counter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fran_counter2['Dragon Ball']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test google serpapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "API_KEY = \"e83711a1e8dfbb65831919301a04eb7ec78aef55d66d64038e5261bff3122442\"  # 替换为你的 API KEY\n",
    "# CX = \"YOUR_SEARCH_ENGINE_ID\"     # 替换为你的 CSE ID\n",
    "\n",
    "def google_serp_search(query):\n",
    "    \"\"\"调用 Google Custom Search API 搜索结果\"\"\"\n",
    "    url = \"https://serpapi.com/search.json\"\n",
    "    params = {\n",
    "        \"api_key\": API_KEY,\n",
    "        \"engine\": \"google\",\n",
    "        \"q\": query\n",
    "    }\n",
    "    resp = requests.get(url, params=params)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n",
    "\n",
    "# query = \"Re:ZERO -Starting Life in Another World- site:fandom.com\"\n",
    "# resp = google_serp_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "fran_community_dict = dict()\n",
    "\n",
    "for fran in fran_counter.keys():\n",
    "    if fran in fran_community_dict:\n",
    "        continue\n",
    "    resp = google_serp_search(f\"{fran} site:fandom.com\")\n",
    "    try:\n",
    "        fran_community_dict[fran] = resp['organic_results']\n",
    "    except:\n",
    "        fran_community_dict[fran] = resp['error']\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'search_metadata': {'id': '68f4866648c7910b423fdb64',\n",
       "  'status': 'Success',\n",
       "  'json_endpoint': 'https://serpapi.com/searches/e1756ce4e4428f3c/68f4866648c7910b423fdb64.json',\n",
       "  'pixel_position_endpoint': 'https://serpapi.com/searches/e1756ce4e4428f3c/68f4866648c7910b423fdb64.json_with_pixel_position',\n",
       "  'created_at': '2025-10-19 06:34:14 UTC',\n",
       "  'processed_at': '2025-10-19 06:34:14 UTC',\n",
       "  'google_url': 'https://www.google.com/search?q=KONOSUBA+-God%27s+blessing+on+this+wonderful+world%21%3A+God%27s+Blessings+On+This+Wonderful+Choker%21+site%3Afandom.com&oq=KONOSUBA+-God%27s+blessing+on+this+wonderful+world%21%3A+God%27s+Blessings+On+This+Wonderful+Choker%21+site%3Afandom.com&sourceid=chrome&ie=UTF-8',\n",
       "  'raw_html_file': 'https://serpapi.com/searches/e1756ce4e4428f3c/68f4866648c7910b423fdb64.html',\n",
       "  'total_time_taken': 1.26},\n",
       " 'search_parameters': {'engine': 'google',\n",
       "  'q': \"KONOSUBA -God's blessing on this wonderful world!: God's Blessings On This Wonderful Choker! site:fandom.com\",\n",
       "  'google_domain': 'google.com',\n",
       "  'device': 'desktop'},\n",
       " 'search_information': {'query_displayed': \"KONOSUBA -God's blessing on this wonderful world!: God's Blessings On This Wonderful Choker! site:fandom.com\",\n",
       "  'total_results': 0,\n",
       "  'time_taken_displayed': 0.11,\n",
       "  'organic_results_state': 'Fully empty',\n",
       "  'query_feedback': {'title': 'Your search did not match any documents'}},\n",
       " 'error': \"Google hasn't returned any results for this query.\"}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(fran_community_dict, \"fran_community_dict_full.json\")\n",
    "\n",
    "link_dict = dict()\n",
    "saved_list = ['title', 'link']\n",
    "saved_num = 5\n",
    "for k, v in fran_community_dict.items():\n",
    "    link_dict[k] = []\n",
    "    for i in range(min(saved_num, len(v))):\n",
    "        item = {x: v[i][x] for x in saved_list}\n",
    "        link_dict[k].append(item)\n",
    "\n",
    "save_json(link_dict, \"fran_community_dict_part.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KONOSUBA -God's blessing on this wonderful world!: God's Blessings On This Wonderful Choker!  not found!!\n",
      "Persona 5 The Animation -the Day Breakers-  not found!!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "pattern = re.compile(r\"https?://([a-zA-Z0-9-]+)\\.fandom\\.com\")\n",
    "\n",
    "best_link_dict = dict()\n",
    "for k, v in fran_community_dict.items():\n",
    "    num_counter = defaultdict(int)\n",
    "    if not isinstance(v, list):\n",
    "        print(k, ' not found!!')\n",
    "        continue\n",
    "    for item in v:\n",
    "        match = pattern.search(item['link'])\n",
    "        com = match.group(1)\n",
    "        num_counter[com] += 1\n",
    "    best_com = max(num_counter, key=num_counter.get)\n",
    "    best_link_dict[k] = f\"https://{best_com}.fandom.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(best_link_dict, \"fran_community_dict_part.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### delete and search individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1107\n"
     ]
    }
   ],
   "source": [
    "def load_json(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "data_latest = load_json(\"./gt/character_fandom_latest.json\")\n",
    "print(len(data_latest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1082\n"
     ]
    }
   ],
   "source": [
    "to_be_removed = load_json('to_be_removed.json')\n",
    "\n",
    "for k in to_be_removed.keys():\n",
    "    del data_latest[k]\n",
    "\n",
    "print(len(data_latest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luffy Monkey (ONE PIECE) 1082\n",
      "Killua Zoldyck (Hunter x Hunter) 1082\n",
      "Kiyotaka Ayanokouji (Classroom of the Elite) 1082\n",
      "Jin-U Seong (Solo Leveling) 1082\n",
      "Yuu Ishigami (Kaguya-sama: Love is War) 1082\n",
      "Gon Freecss (Hunter x Hunter) 1082\n",
      "Shinobu Oshino (Nekomonogatari Black) 1082\n",
      "Spike Spiegel (Cowboy Bebop) 1082\n",
      "Yuu Nishinoya (HAIKYU!!) 1082\n",
      "Ace Portgas (ONE PIECE) 1082\n",
      "Anna Liebert (Monster) 1082\n",
      "Riki Nendou (The Disastrous Life of Saiki K.) 1082\n",
      "Gohan Son (Dragon Ball GT) 1082\n",
      "Chuuya Nakahara (Bungo Stray Dogs) 1082\n",
      "Shinoa Hiiragi (Seraph of the End: Vampire Reign) 1082\n",
      "Ken Takakura (Dandadan) 1082\n",
      "Yuuri Plisetsky (Yuri!!! on ICE) 1082\n",
      "Chiaki Nanami (Danganronpa 2: Chiaki Nanami's Goodbye Despair Quest) 1082\n",
      "YoRHa 2-gou B-gata (NieR:Automata: Long Story Short) 1082\n",
      "Ekubo (Mob Psycho 100) 1082\n",
      "Jirou Souzousa Shunsui Kyouraku (Bleach) 1082\n",
      "Tsunade Senju (Naruto) 1082\n",
      "San (Princess Mononoke) 1082\n",
      "Fyodor Dostoyevsky (Bungo Stray Dogs 3) 1082\n",
      "Kyouka Izumi (Bungo Stray Dogs) 1082\n",
      "Kuro Nero (Black Clover) 1082\n",
      "Yutaka Hoshino (Ping Pong the Animation) 1082\n",
      "Aurora Suya Rhys Kaymin (Sleepy Princess in the Demon Castle) 1082\n",
      "Ushikai Musume (GOBLIN SLAYER) 1082\n",
      "Shouei Barou (Blue Lock) 1082\n",
      "Kaonashi (Spirited Away) 1082\n",
      "Asuka Langley Souryuu (Evangelion: 2.0 You Can (Not) Advance) 1082\n",
      "Chousou (Jujutsu Kaisen) 1082\n"
     ]
    }
   ],
   "source": [
    "from fandom_character_info import parse_character_page\n",
    "\n",
    "to_be_updated = load_json('re_search.json')\n",
    "for char, url in to_be_updated.items():\n",
    "    data_latest[char] = parse_character_page(url)\n",
    "    print(char, len(data_latest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(data_latest, \"./gt/character_fandom.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRCharacter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
