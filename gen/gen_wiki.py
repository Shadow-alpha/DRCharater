import json
import requests
import time
from datetime import datetime
import os
import pandas as pd
import pdb
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from utils import get_response, save_result, save_result_txt, extract_json, ensure_question_format
import random 
from utils import set_cache_path, init_writer, close_writer

# é…ç½®æ–¹æ³•é€‰æ‹©
search_model = 'gemini_search' # 'gemini_search' or 'doubao_search'
profiling_model = 'qwen'
language = 'en'  # choose 'zh' or 'en'
if_translated = True
translate_model = "qwen"

# results_file = "./results/gemini_search/gemini_profile.json"
# results_file = "./results/doubao_search_acg_characters_v1_output_20251026_141356.json"

entity_files = ['../getcharacter/acg_characters_v1.jsonl']
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
output_file = f'{search_model}_acg_characters_v1_output_{timestamp}.json'
existing_files = []
parallel = True
max_workers = 3

set_cache_path('.cache-acg.pkl') # '.cache-' + output_file.replace('.json', '.pkl'))

progress_count = 0
progress_lock = threading.Lock()
total_entities = 0
save_interval = 10  # æ¯10ä¸ªå®ä½“ä¿å­˜ä¸€æ¬¡

def to_my_entity_key(entity_info):
	if 'entity_info' in entity_info:
		return entity_info['entity_info']['label'] + f' ({entity_info["franchise"]})'
	else:
		return  entity_info['label'] + f' ({entity_info["franchise"]})'

def save_progress(results, filename=None):
	"""ç»Ÿä¸€çš„ä¿å­˜è¿›åº¦å‡½æ•°"""
	if filename is None:
		filename = output_file
	
	os.makedirs("results", exist_ok=True)
	with open(f'results/{filename}', 'w', encoding='utf-8') as f:
		json.dump(results, f, ensure_ascii=False, indent=2)

def process_entity(entity_info):
	"""å¤„ç†å•ä¸ªå®ä½“çš„å‡½æ•°ï¼Œç”¨äºå¹¶å‘æ‰§è¡Œ"""
	global progress_count, total_entities

	entity_name = entity_info['label']
	entity_description = f'{entity_info["franchise"]}'
	
	with progress_lock:
		progress_count += 1
		current = progress_count
	
	print(f"[{current}/{total_entities}] å¼€å§‹æŸ¥è¯¢å®ä½“: {entity_name}")
	
	# ä¿å­˜å®Œæ•´çš„å®ä½“ä¿¡æ¯
	result = {
		'entity': entity_name,
		'entity_info': entity_info.to_dict() if hasattr(entity_info, 'to_dict') else entity_info
	} 

	pre_result = total_results.get(to_my_entity_key(entity_info), None)
	
	if search_model == "gemini_search":
		from prompts import get_prompt

		messages = []

		# æœé›†å®ä½“ä¿¡æ¯ - ç¬¬ä¸€æ¬¡ä½¿ç”¨label + description
		search_prompt = get_prompt('search_prompt', language) + "You should include the character's personality (very important), background, physical description, core motivations, notable attributes, relationships, key experiences, major plot involvement and key decisions or actions, character arc or development throughout the story, if there is any information about these aspects."
		entity_full = f"{entity_name} ({entity_description})" if entity_description else entity_name
		prompt = search_prompt.replace('{entity}', entity_full, 1).replace('{entity}', entity_name)
		messages.append({'role': 'user', 'content': prompt})
		if pre_result and pre_result.get('search_response', None):
			knowledge = pre_result['search_response']
			print('search response already exist')
		else:
			knowledge = get_response(model=search_model, messages=messages)
		result['search_response'] = knowledge
		messages.append({'role': 'assistant', 'content': knowledge})

		if knowledge is None:
			return result

		# äºŒæ¬¡æ‰©å±•
		search_second_prompt = get_prompt('search_second_prompt', language)
		messages.append({'role': 'user', 'content': search_second_prompt})
		if pre_result and pre_result.get('search_again_response', None):
			knowledge2 = pre_result['search_again_response']
			print('search again response already exist')
		else:
			knowledge2 = get_response(model=search_model, messages=messages)
		result['search_again_response'] = knowledge2
		messages.append({'role': 'assistant', 'content': knowledge2})

		if knowledge2 is None:
			return result

	elif search_model == "doubao_search":
		from prompts import PROMPT_DEEP_SEARCH
		messages = []

		# æœé›†å®ä½“ä¿¡æ¯ - ç¬¬ä¸€æ¬¡ä½¿ç”¨label + description
		search_prompt = PROMPT_DEEP_SEARCH[language]
		entity_full = f"{entity_name} ({entity_description})" if entity_description else entity_name
		prompt = search_prompt.replace('{entity}', entity_full, 1).replace('{entity}', entity_name)
		messages.append({'role': 'user', 'content': prompt})
		if pre_result and pre_result.get('search_response', None):
			knowledge = pre_result['search_response']
			print('search response already exist')
		else:
			knowledge = get_response(model=search_model, messages=messages)
		result['search_response'] = knowledge
		messages.append({'role': 'assistant', 'content': knowledge})

		if knowledge is None:
			return result

	# ç”Ÿæˆé—®é¢˜ - åç»­åªä½¿ç”¨label
	if language == 'en':
		profiling_prompt = "Please completely rewrite all the above information from {entity}'s first-person perspective. Ensure that the information is comprehensive and accurate. You need to focus on the character's personality, which you could also analyze based on the characters' experiences. Besides, include include the character's, background, physical description, core motivations, notable attributes, relationships, key experiences, major plot involvement and key decisions or actions, character arc or development throughout the story, and other important details, if they appear in the information that you obtained."
	elif language == 'zh':
		profiling_prompt = "è¯·å°†ä¸Šè¿°æ‰€æœ‰ä¿¡æ¯å®Œå…¨æ”¹å†™ä¸ºä»¥ {entity} çš„ç¬¬ä¸€äººç§°è§†è§’å™è¿°çš„å½¢å¼ã€‚ç¡®ä¿ä¿¡æ¯å…¨é¢ä¸”å‡†ç¡®ã€‚ä½ éœ€è¦é‡ç‚¹æå†™è§’è‰²çš„æ€§æ ¼ï¼Œä¹Ÿå¯ä»¥ç»“åˆè§’è‰²çš„ç»å†è¿›è¡Œåˆ†æã€‚æ­¤å¤–ï¼Œå¦‚æœä¿¡æ¯ä¸­æœ‰ï¼Œè¿˜åº”åŒ…æ‹¬è§’è‰²çš„èƒŒæ™¯å‡ºèº«ã€å¤–è²Œæå†™ã€æ ¸å¿ƒåŠ¨æœºã€æ˜¾è‘—ç‰¹å¾ã€äººé™…å…³ç³»ã€å…³é”®ç»å†ã€ä¸»è¦å‰§æƒ…å‚ä¸å’Œé‡è¦å†³ç­–æˆ–è¡ŒåŠ¨ã€è§’è‰²å¼§çº¿æˆ–åœ¨æ•…äº‹ä¸­çš„å‘å±•ï¼Œä»¥åŠä½ æ‰€è·å–ä¿¡æ¯ä¸­å‡ºç°çš„å…¶ä»–é‡è¦ç»†èŠ‚ã€‚"
	
	profiling_prompt = profiling_prompt.replace('{entity}', entity_name)
		
	messages.append({'role': 'user', 'content': profiling_prompt})

	if language == 'en':
		k1, k2 = 'english_profile', 'chinese_profile'
	elif language == 'zh':
		k1, k2 = 'chinese_profile', 'english_profile'
	
	if pre_result and pre_result.get(k1, None):
			profile = pre_result[k1]
			print(f'{k1} already exist')
	else:
		profile = get_response(model=profiling_model, messages=messages)
		
	result[k1] = profile

	if profile is None:
		return result

	if if_translated:
		if language == 'en':
			translation_prompt = "æˆ‘ä¼šç»™ä½ ä¸€æ®µå…³äº{entity}çš„ç¬¬ä¸€äººç§°è§†è§’çš„è‡ªæˆ‘ä»‹ç»ã€‚ä½ è¯·å°†å®ƒå¿ å®åœ°ç¿»è¯‘ä¸ºä¸­æ–‡ã€‚ä¸è¦é—æ¼ä»»ä½•ä¿¡æ¯ã€‚"
		elif language == 'zh':
			translation_prompt = "I will provide you with a self-introduction written from the first-person perspective of {entity}. Please translate it into English faithfully, without omitting any information."
		translation_prompt = translation_prompt.replace('{entity}', entity_name) + '\n\n' + profile

		if pre_result and pre_result.get(k2, None):
			translated_profile = pre_result[k2]
			print(f'{k2} already exist')
		else:
			translated_profile = get_response(model=translate_model, messages=[{'role': 'user', 'content': translation_prompt}])

		result[k2] = translated_profile

	# result['chinese_profile'] = zh_profile
	# 	# ç¦»çº¿åˆ¤å®šç­”æ¡ˆå”¯ä¸€æ€§ TODO
	return result

def load_file(path: str):
	data = []
	if path.endswith('.csv'):
		df = pd.read_csv(path)
		for _, row in df.iterrows():
			entity_dict = row.to_dict()
			data.append(entity_dict)

	elif path.endswith('.json'):
		with open(path, 'r', encoding='utf-8') as f:
			json_data = json.load(f)
			if isinstance(json_data, list):
				data = json_data
			elif isinstance(json_data, dict):
				data = [v['entity_info'] for k, v in json_data.items()]
			else:
				raise NotImplementedError(f'can not parse file {path}')
	else:
		raise NotImplementedError(f'can not parse file {path}')
	
	print(f"æˆåŠŸè¯»å– {path}ï¼Œå…± {len(data)} æ¡è®°å½•")
	return data

try:
	with open(results_file, 'r', encoding='utf-8') as f:
		total_results: dict = json.load(f)
except:
	total_results = dict()

def main():
	global total_entities
	
	# è¯»å–å¤šä¸ªå®ä½“æ–‡ä»¶
	entities_data = []
	n_entities = 10100
	# æŒ‰ä¼˜å…ˆçº§é¡ºåºè¯»å–å„ä¸ªæ–‡ä»¶
	for i_f, entity_file in enumerate(entity_files):
		_entities_data = load_file(entity_file)
		# df = pd.read_csv(entity_file)
		# print(f"æˆåŠŸè¯»å– {entity_file}ï¼Œå…± {len(df)} æ¡è®°å½•")
		
		# è¯»å–å®ä½“ä¿¡æ¯ï¼Œæ·»åŠ åˆ°åˆ—è¡¨ä¸­ï¼Œæ’é™¤å·²æœ‰å®ä½“
		# _entities_data = []
		# for _, row in df.iterrows():
		# 	entity_dict = row.to_dict()
		# 	_entities_data.append(entity_dict)
		
		entities_data.extend(_entities_data)

	# å±•ç¤ºpopularityåˆ†å¸ƒ
	entities_data = entities_data
	print(f"æ€»å…±è¯»å–äº† {len(entities_data)} ä¸ªå®ä½“")
	
	total_entities = len(entities_data)

	# init_writer(f"results/{search_model}/{timestamp}_response.jsonl")
	# æ ¹æ®æ–¹æ³•è°ƒæ•´å¹¶å‘æ•°
	
	# åˆå§‹åŒ–ç»“æœå­—å…¸ï¼ŒåŒ…å«å·²æœ‰ç»“æœå’Œæ–°å®ä½“
	results = {}

	if parallel:
		completed_count = 0
		with ThreadPoolExecutor(max_workers=max_workers) as executor:
			# æäº¤æ‰€æœ‰ä»»åŠ¡
			future_to_entity = {executor.submit(process_entity, entity_info): to_my_entity_key(entity_info) for entity_info in entities_data}
			
			# å¤„ç†å®Œæˆçš„ä»»åŠ¡
			for future in as_completed(future_to_entity):
				entity_key = future_to_entity[future]
				result = future.result()
				results[entity_key] = result
				completed_count += 1
				
				# æ¯100ä¸ªå®ä½“ä¿å­˜ä¸€æ¬¡
				if completed_count % save_interval == 0:
					print(f"ğŸ’¾ å·²å®Œæˆ {completed_count} ä¸ªå®ä½“ï¼Œä¿å­˜ä¸­é—´ç»“æœ...")
					save_progress(results)
					print(f"ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜: results/{output_file}")

	else:
		for i, entity_info in enumerate(entities_data, 1):
			result = process_entity(entity_info)
			results[to_my_entity_key(entity_info)] = result
			
			# æ¯100ä¸ªå®ä½“ä¿å­˜ä¸€æ¬¡
			if i % save_interval == 0:
				print(f"ğŸ’¾ å·²å®Œæˆ {i} ä¸ªå®ä½“ï¼Œä¿å­˜ä¸­é—´ç»“æœ...")
				save_progress(results)
				print(f"ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜: results/{output_file}")

	# ç»Ÿè®¡ç»“æœï¼šå·²æœ‰æ•°æ® + æ–°å®Œæˆçš„æ•°æ®
	total_completed = len([result for result in results.values() if result is not None])
	new_completed = len([result for entity_info in entities_data for result in [results[to_my_entity_key(entity_info)]] if result is not None])
	new_failed = len(entities_data) - new_completed
	
	print(f"ğŸ“Š ç»Ÿè®¡ç»“æœ:")
	# print(f"  å·²æœ‰å®ä½“: {len(existing_entitiy_keys)}")
	print(f"  æ–°å¤„ç†å®ä½“: {len(entities_data)}")
	print(f"  æ–°æˆåŠŸ: {new_completed}, æ–°å¤±è´¥: {new_failed}")
	print(f"  æ€»è®¡æˆåŠŸ: {total_completed}, æ€»è®¡å®ä½“: {len(results)}")

	# ä¿å­˜æ±‡æ€»ç»“æœ
	save_progress(results, output_file)

	# ä¿å­˜TXTæ ¼å¼çš„ç®€åŒ–ç»“æœ
	save_result_txt(f'results/{output_file}_simple.txt', results)

	print(f"ğŸ“ ç»“æœä¿å­˜åœ¨ results/ ç›®å½•ä¸‹ï¼ˆä½¿ç”¨{profiling_model}æ–¹æ³•ï¼‰")
	print(f"ğŸ“„ JSONæ ¼å¼: results/{output_file}.json")
	print(f"ğŸ“„ TXTæ ¼å¼: results/{output_file}_simple.txt")

	# close_writer()

if __name__ == "__main__":
	main()
