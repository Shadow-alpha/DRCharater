import pdb 
import os
import re 
import random 
import openai
import json
import logging
import time  
# import jsonlines 
import requests 
import io
import pickle
import random
import __main__
import tiktoken
import threading
from typing import Dict, List
import pandas as pd
# import google

with open('config.json', 'r') as f:
	config = json.load(f)

streaming = False

def load_file(path: str):
	if path.endswith('.json'):
		with open(path, 'r', encoding='utf-8') as f:
			data = json.load(f)
	elif path.endswith('.jsonl'):
		with open(path, 'r', encoding='utf-8') as f:
			data = [json.loads(line) for line in f.readlines()]
	elif path.endswith('.csv'):
		data = pd.read_csv(path)
		data = [row.to_dict() for i,row in data.iterrows()]
	else:
		raise NameError('the file not supported')
	return data

def setup_logger(name, log_file, level=logging.INFO, quiet=False):
	logger = logging.getLogger(name)
	logger.setLevel(level)

	if logger.hasHandlers():
		logger.handlers.clear()

	file_handler = logging.FileHandler(log_file, encoding='utf-8')
	file_handler.setLevel(logging.DEBUG)
	file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
	file_handler.setFormatter(file_formatter)
	logger.addHandler(file_handler)

	if not quiet:
		console_handler = logging.StreamHandler()
		console_handler.setLevel(level)
		console_formatter = logging.Formatter('%(name)s - %(levelname)s - %(message)s [%(filename)s:%(lineno)d]')
		console_handler.setFormatter(console_formatter)
		logger.addHandler(console_handler)

	return logger

logger = setup_logger(__name__, f'{__file__.split(".")[0]}.log', level=logging.INFO, quiet=False)

from contextlib import contextmanager
import tempfile
@contextmanager
def _tempfile(dir=None,*args, **kws):
	""" Context for temporary file.
	Will find a free temporary filename upon entering
	and will try to delete the file on leaving
	Parameters
	----------
	suffix : string
		optional file suffix
	dir : string
		directory to create temp file in, will be created if doesn't exist
	"""
	if dir is not None:
		os.makedirs(dir, exist_ok=True)
		
	fd, name = tempfile.mkstemp(dir=dir, *args, **kws)
	os.close(fd)
	try:
		yield name
	finally:
		try:
			os.remove(name)
		except OSError as e:
			if e.errno == 2:
				pass
			else:
				raise e
			
@contextmanager
def open_atomic(filepath, *args, **kwargs):
	""" Open temporary file object that atomically moves to destination upon
	exiting.
	Allows reading and writing to and from the same filename.
	Parameters
	----------
	filepath : string
		the file path to be opened
	fsync : bool
		whether to force write the file to disk
	kwargs : mixed
		Any valid keyword arguments for :code:`open`
	"""
	fsync = kwargs.pop('fsync', False)

	original_permissions = os.stat(filepath).st_mode if os.path.exists(filepath) else None 

	with _tempfile(dir=os.path.join(os.path.dirname(filepath), 'temp')) as tmppath:
		with open(tmppath, *args, **kwargs) as f:
			yield f
			if fsync:
				f.flush()
				os.fsync(f.fileno())
		try:
			os.rename(tmppath, filepath)
		except:
			pass
		if original_permissions is not None:
			os.chmod(filepath, original_permissions)

import datetime
def convert_to_timestamp(time_str: str):
	return time.mktime(datetime.datetime.strptime(time_str, "%Y-%m-%d").timetuple())

def safe_pickle_dump(obj, fname):
	"""
	prevents a case where one process could be writing a pickle file
	while another process is reading it, causing a crash. the solution
	is to write the pickle file to a temporary file and then move it.
	"""
	with open_atomic(fname, 'wb') as f:
		pickle.dump(obj, f, -1) # -1 specifies highest binary protocol


ERROR_SIGN = '[ERROR]'

cache_path = config['cache']['default_path']
cache_sign = True
cache = None
reload_cache = False
cache_lock = threading.Lock()  # æ·»åŠ çº¿ç¨‹é”

def set_cache_path(new_cache_path):
	global cache_path
	cache_path = new_cache_path
	global reload_cache
	reload_cache = True
	print(f"set cache path to {cache_path}")

def cached(func):
	def wrapper(*args, **kwargs):		
		# extract_from_chunk 
		if func.__name__ == 'extract_from_chunk':
			key = ( func.__name__, args[0]['title'], args[1]) 
		else:
			key = ( func.__name__, str(args), str(kwargs.items())) 

		global cache
		global reload_cache

		# ä½¿ç”¨çº¿ç¨‹é”ä¿æŠ¤ç¼“å­˜æ“ä½œ
		with cache_lock:
			if reload_cache:
				cache = None # to reload
				reload_cache = False

			if cache == None:
				if not os.path.exists(cache_path):
					cache = {}
				else:
					try:
						cache = pickle.load(open(cache_path, 'rb'))  
					except Exception as e:
						# logger.info cache_path and throw error
						logger.error(f'Error loading cache from {cache_path}')
						cache = {}

			if (cache_sign and key in cache) and not (cache[key] is None) and (not cache[key] == ERROR_SIGN):
				return cache[key]

		# åœ¨é”å¤–æ‰§è¡Œå‡½æ•°è°ƒç”¨ï¼ˆé¿å…é•¿æ—¶é—´æŒæœ‰é”ï¼‰
		result = func(*args, **kwargs)
		
		# é‡æ–°è·å–é”æ¥æ›´æ–°ç¼“å­˜
		if result != None:
			with cache_lock:
				cache[key] = result
				# åˆ›å»ºç¼“å­˜å‰¯æœ¬é¿å…åœ¨ä¿å­˜æ—¶è¢«ä¿®æ”¹
				cache_copy = cache.copy()
			# åœ¨é”å¤–ä¿å­˜æ–‡ä»¶
			safe_pickle_dump(cache_copy, cache_path)
		
		return result

	return wrapper

enc = tiktoken.get_encoding(config['encoding']['name'])

def encode(text):
	return enc.encode(text)

def decode(tokens):
	return enc.decode(tokens)

def num_tokens_from_string(string: str, encoding_name: str = "cl100k_base") -> int:
	encoding = tiktoken.get_encoding(encoding_name)
	num_tokens = len(encoding.encode(string))
	logger.info(f"Number of tokens: {num_tokens}")
	return num_tokens

def gemini(messages, search=False):
	# è½¬æ¢æˆgoogle apiæ”¯æŒçš„æ•°æ®æ ¼å¼
	def convert_google_message(messages):
		results = []
		for message in messages:
			result = {'role': message['role'], 'parts':[{'text': message['content']}]}
			if message['role'] == 'assistant':
				result['role'] = 'model'
			results.append(result)
		return results

	"""ä½¿ç”¨ç°æœ‰çš„gemini search API"""
	# ä»é…ç½®æ–‡ä»¶è·å–APIé…ç½®
	gemini_config = config['gemini_search']
	url = gemini_config['url']
	# params = {
	# 	"ak": gemini_config['ak']
	# }
	
	# è¯·æ±‚å¤´
	headers = {
		"Content-Type": "application/json",
		"x-goog-api-key": gemini_config['ak']
	}
	
	# è¯·æ±‚æ•°æ®
	model = gemini_config['model']
	data = {
		"model": model, 
		"contents": convert_google_message(messages)
	}

	if search:
		data['tools'] = [
			{
				"google_search": {}
			}
		]
	
	try:
		response = requests.post(
			url=url,
			headers=headers,
			json=data,
			timeout=gemini_config['timeout']
		)
		try:
			return ''.join([res['text'] for res in response.json()['candidates'][0]['content']['parts']])
			# return response.json()['choices'][0]['message']['content']
		except Exception as e:
			time.sleep(30)
			logger.error(f"Error parsing response: {response.text}")

			# if any(word in response.text.lower() for word in ['adult', 'prohibited', 'blocked']):
			# 	return ERROR_SIGN
			# else:
			# 	return None

			if any(word in response.text.lower() for word in ['limit', 'resource', 'timeout', 'time out', 'try again']):
				return None
			else:
				return ERROR_SIGN
			
	except Exception as e:
		print(f"è¯·æ±‚å¤±è´¥: {e}")
		time.sleep(30)
		return None

def claude(messages):
	"""ä½¿ç”¨ç°æœ‰çš„claude API"""
	# ä»é…ç½®æ–‡ä»¶è·å–APIé…ç½®
	claude_config = config['claude']

# è¯·æ±‚æ•°æ®
	client = openai.OpenAI(
		api_key=claude_config['ak'],
		base_url=claude_config['url']
	)

	request_params  = {
			"model": claude_config['model'],
			"messages": messages
		}

	try:
		response = client.chat.completions.create(**request_params)

		return response.choices[0].message.content
			
	except Exception as e:
		print(f"è¯·æ±‚å¤±è´¥: {e}")
		return None

def gpt(messages):
	"""ä½¿ç”¨ç°æœ‰çš„claude API"""
	# ä»é…ç½®æ–‡ä»¶è·å–APIé…ç½®
	gpt_config = config['gpt']
	# print(gpt_config)

	# è¯·æ±‚æ•°æ®
	client = openai.OpenAI(
		api_key=gpt_config['ak'],
		base_url=gpt_config['url']
	)

	request_params  = {
			"model": gpt_config['model'],
			"messages": messages
		}

	try:
		response = client.chat.completions.create(**request_params)
		return response.choices[0].message.content
	
		# response_dict = response.model_dump()
		# return response_dict
			
	except Exception as e:
		print(f"è¯·æ±‚å¤±è´¥: {e}")
		return None
	
def deer_flow(messages):
	"""ä½¿ç”¨deer-flow APIï¼ˆå‡è®¾æœ¬åœ°è¿è¡Œï¼‰"""
	# ä»é…ç½®æ–‡ä»¶è·å–deer-flowé…ç½®
	deer_config = config['deer_flow']
	deer_flow_url = deer_config['url']
	
	try:
		# æ„å»ºdeer-flowçš„è¯·æ±‚æ ¼å¼
		data = {
			"messages": messages,
			"auto_accepted_plan": deer_config['auto_accepted_plan'],
			"max_step_num": deer_config['max_step_num']
		}
		
		headers = {
			"Content-Type": "application/json"
		}
		
		print(f"æ­£åœ¨ä½¿ç”¨deer-flowå¤„ç†: {messages[0]['content'][:50] if messages and 'content' in messages[0] else 'request'}...")
		response = requests.post(
			url=deer_flow_url,
			headers=headers,
			json=data,
			timeout=deer_config['timeout']
		)
		
		if response.status_code == 200:
			return response.json()
		else:
			print(f"Deer-flow APIé”™è¯¯ï¼ŒçŠ¶æ€ç : {response.status_code}")
			return None
			
	except requests.exceptions.ConnectionError:
		print("é”™è¯¯: æ— æ³•è¿æ¥åˆ°deer-flowæœåŠ¡ã€‚è¯·ç¡®ä¿deer-flowæ­£åœ¨localhost:8000è¿è¡Œã€‚")
		print("å¯åŠ¨deer-flowå‘½ä»¤: cd deer-flow && python server.py")
		return None
	except Exception as e:
		print(f"Deer-flowè¯·æ±‚å¤±è´¥: {e}")
		return None

def qwen(messages, search=False):

    config_qwen = config['qwen']
    # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
    client = openai.OpenAI(
        # å¦‚æœæ²¡æœ‰é…ç½®ç¯å¢ƒå˜é‡ï¼Œè¯·ç”¨é˜¿é‡Œäº‘ç™¾ç‚¼API Keyæ›¿æ¢ï¼šapi_key="sk-xxx"
        api_key=config_qwen['ak'],
        base_url=config_qwen['url'],
    )

    try:
        completion = client.chat.completions.create(
            model=config_qwen['model'],  # æ‚¨å¯ä»¥æŒ‰éœ€æ›´æ¢ä¸ºå…¶å®ƒæ·±åº¦æ€è€ƒæ¨¡å‹
            messages=messages,
            # enable_thinking å‚æ•°å¼€å¯æ€è€ƒè¿‡ç¨‹ï¼Œqwen3-30b-a3b-thinking-2507ã€qwen3-235b-a22b-thinking-2507ã€QwQ ä¸ DeepSeek-R1 æ¨¡å‹æ€»ä¼šè¿›è¡Œæ€è€ƒï¼Œä¸æ”¯æŒè¯¥å‚æ•°
            extra_body={"enable_search": search},
            # stream=True,
            # stream_options={
            #     "include_usage": True
            # },
        )
        return completion.choices[0].message.content
    
    except Exception as e:
        print(f"è¯·æ±‚å¤±è´¥: {e}")
        time.sleep(10)
        return None


def doubao(messages, search=False):
	## è±†åŒ…å¤§æ¨¡å‹api
	# "0544d39a-ea9e-4298-ad0a-c98029ed4eb7"
	# "https://ark.cn-beijing.volces.com/api/v3/chat/completions"
	doubao_config = config['doubao']

	headers = {
		"Content-Type": "application/json",
		"Authorization": f"Bearer {doubao_config['ak']}",
		"ark-beta-web-search": str(search)
	}

	data = {
		"model": doubao_config['model'],
		"input": messages
	}

	if search:
		data['tools'] = [
			{"type": "web_search"}
		]

	try:
		response = requests.post(
			url=doubao_config['url'],
			headers=headers,
			json=data,
			timeout=240
		)
		try:
			write_jsonl(response.json())
		except Exception as e:
			print('fail to write response in ' + _file_handle.name)

		return response.json()['output'][-1]['content'][0]['text']
			
	except Exception as e:
		time.sleep(5)
		print(f"è¯·æ±‚å¤±è´¥: {e}")
		return ERROR_SIGN

# å…¨å±€æ–‡ä»¶å¥æŸ„å’Œé”
_file_lock = threading.Lock()
_file_handle = None

def init_writer(file_path="llm_responses.jsonl"):
	"""åˆå§‹åŒ–å…¨å±€æ–‡ä»¶å¥æŸ„"""
	global _file_handle
	if not os.path.exists(os.path.dirname(file_path)):
		os.makedirs(os.path.dirname(file_path))
	if _file_handle is None:
		_file_handle = open(file_path, "a", encoding="utf-8")
		
def close_writer():
    """å…³é—­å…¨å±€æ–‡ä»¶å¥æŸ„"""
    global _file_handle
    if _file_handle:
        _file_handle.close()
        _file_handle = None

def write_jsonl(record: dict):
    """çº¿ç¨‹å®‰å…¨å†™å…¥ä¸€æ¡è®°å½•"""
    global _file_handle
    line = json.dumps(record, ensure_ascii=False) + "\n"
    with _file_lock:  # ç¡®ä¿åŒä¸€æ—¶é—´åªæœ‰ä¸€ä¸ªçº¿ç¨‹å†™
        _file_handle.write(line)
        _file_handle.flush()

@cached
def _get_response(model, messages, nth_generation=0, **kwargs):
	# if messages is str
	if isinstance(messages, str):
		messages = [{"role": "user", "content": messages}]

	try:
		if model == 'gemini_search': 
			response = gemini(messages, search=True)
		elif model == 'gemini':
			response = gemini(messages)
		elif model == 'doubao_search':
			response = doubao(messages, search=True)
		elif model.startswith('claude'):
			response = claude(messages)
		elif model.startswith('gpt'):
			response = gpt(messages)
		elif model.startswith('qwen'):
			response = qwen(messages, search=False)
		elif model == 'deer-flow':
			pass
		
		return response

	except Exception as e:
		import traceback 
		logger.error(f'Prompt: {messages[:500]}')
		logger.error(f"Error in _get_response: {str(e)}")

		try:
			if hasattr(response, 'text'):
				logger.error(f"Response: {response.text}")
			else:
				logger.error(f"Response: {response}")
		except Exception as e:
			logger.error(f"Could not print response: {e}")
		
		logger.error(f"Number of input tokens: {num_tokens_from_string(messages[0]['content'])}")

		traceback.print_exc()
		return None

def get_response(post_processing_funcs=[], **kwargs):
	nth_generation = 0

	while True:
		if nth_generation > kwargs.get('max_retry', 3):
			# Return error response with backup data if parse_response failed
			return None
		
		logger.info(f'{nth_generation}th generation')
		response = _get_response(**kwargs, nth_generation=nth_generation)
		logger.info(f'response by LLM: {response[:1000] if isinstance(response, str) else response}')

		if response is None:
			nth_generation += 1
			continue 
		
		if response == ERROR_SIGN: # BLOCKED
			nth_generation += 1
			continue

		# Break if we got a valid response, otherwise retry
		# Run response through post-processing pipeline
		for i, post_processing_func in enumerate(post_processing_funcs):
			if response is None:
				break
			response = post_processing_func(response, **kwargs)

		if response:
			return response
		else:
			nth_generation += 1
			

def ensure_question_format(response, **kwargs):
	try:
		assert isinstance(response, dict)
		#assert 'entity' in response 
		assert 'questions' in response
		for q in response['questions']:
			assert isinstance(q, dict)
			assert 'entity_type' in q
			
		return response
	except:
		return False



def save_result(filename, result):
	"""ä¿å­˜æŸ¥è¯¢ç»“æœåˆ°æ–‡ä»¶"""
	# åˆ›å»ºresultsç›®å½•
	os.makedirs(os.path.dirname(filename), exist_ok=True)
	
	with open(filename, 'w', encoding='utf-8') as f:
		json.dump(result, f, ensure_ascii=False, indent=2)

def format_json_for_display(data, indent_level=0):
	"""
	å°†JSONæ•°æ®æ ¼å¼åŒ–ä¸ºæ˜“è¯»çš„æ–‡æœ¬æ ¼å¼
	"""
	indent = "  " * indent_level
	lines = []
	
	if isinstance(data, dict):
		if not data:
			return "{ }"
		lines.append("{")
		for i, (key, value) in enumerate(data.items()):
			formatted_value = format_json_for_display(value, indent_level + 1)
			comma = "," if i < len(data) - 1 else ""
			lines.append(f"{indent}  \"{key}\": {formatted_value}{comma}")
		lines.append(f"{indent}}}")
		
	elif isinstance(data, list):
		if not data:
			return "[ ]"
		lines.append("[")
		for i, item in enumerate(data):
			formatted_item = format_json_for_display(item, indent_level + 1)
			comma = "," if i < len(data) - 1 else ""
			lines.append(f"{indent}  {formatted_item}{comma}")
		lines.append(f"{indent}]")
		
	elif isinstance(data, str):
		# å¤„ç†é•¿å­—ç¬¦ä¸²ï¼Œæ·»åŠ æ¢è¡Œ
		if len(data) > 80:
			return f'"{data[:77]}..."'
		return f'"{data}"'
		
	elif isinstance(data, (int, float)):
		return str(data)
		
	elif isinstance(data, bool):
		return "true" if data else "false"
		
	elif data is None:
		return "null"
		
	else:
		return f'"{str(data)}"'
	
	return "\n".join(lines)

def save_result_txt(filename, results):
	with open(filename, 'w', encoding='utf-8') as f:
		f.write("=" * 80 + "\n\n")
		
		successful_count = 0
		failed_count = 0
		
		for i, (entity, result) in enumerate(results.items(), 1):
			f.write("â”Œ" + "â”€" * 78 + "â”\n")
			
			if result:
				successful_count += 1
				f.write("â”‚ âœ… çŠ¶æ€: æŸ¥è¯¢æˆåŠŸ\n")
				f.write("â”œ" + "â”€" * 78 + "â”¤\n")
				
				# æ£€æŸ¥resultæ˜¯å¦ä¸ºå­—å…¸æˆ–åˆ—è¡¨ï¼Œå¦‚æœæ˜¯åˆ™ç¾åŒ–è¾“å‡º
				if isinstance(result, (dict, list)):
					f.write("â”‚ ğŸ“‹ æ•°æ®ç±»å‹: JSONç»“æ„\n")
					f.write("â”œ" + "â”€" * 78 + "â”¤\n")
					formatted_json = format_json_for_display(result)
					# ä¸ºæ¯è¡Œæ·»åŠ è¾¹æ¡†
					for line in formatted_json.split('\n'):
						f.write(f"â”‚ {line:<76} â”‚\n")
				else:
					# å¦‚æœresultæ˜¯å­—ç¬¦ä¸²ä½†åŒ…å«JSONï¼Œå°è¯•è§£æå¹¶ç¾åŒ–
					try:
						parsed_result = json.loads(result)
						f.write("â”‚ ğŸ“‹ æ•°æ®ç±»å‹: è§£æåçš„JSONç»“æ„\n")
						f.write("â”œ" + "â”€" * 78 + "â”¤\n")
						formatted_json = format_json_for_display(parsed_result)
						for line in formatted_json.split('\n'):
							f.write(f"â”‚ {line:<76} â”‚\n")
					except (json.JSONDecodeError, TypeError):
						# å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼Œç›´æ¥è¾“å‡ºåŸå†…å®¹
						f.write("â”‚ ğŸ“‹ æ•°æ®ç±»å‹: æ–‡æœ¬å†…å®¹\n")
						f.write("â”œ" + "â”€" * 78 + "â”¤\n")
						# å¤„ç†é•¿æ–‡æœ¬ï¼ŒæŒ‰è¡Œåˆ†å‰²
						content_lines = str(result).split('\n')
						for line in content_lines:
							if len(line) > 76:
								# é•¿è¡Œéœ€è¦æ¢è¡Œ
								words = line.split(' ')
								current_line = ""
								for word in words:
									if len(current_line + " " + word) <= 76:
										current_line += (" " + word) if current_line else word
									else:
										if current_line:
											f.write(f"â”‚ {current_line:<76} â”‚\n")
										current_line = word
								if current_line:
									f.write(f"â”‚ {current_line:<76} â”‚\n")
							else:
								f.write(f"â”‚ {line:<76} â”‚\n")
			else:
				failed_count += 1
				f.write("â”‚ âŒ çŠ¶æ€: æŸ¥è¯¢å¤±è´¥\n")
				f.write("â”œ" + "â”€" * 78 + "â”¤\n")
				f.write("â”‚ ğŸ’¬ åŸå› : æ— æ³•è·å–ç›¸å…³ä¿¡æ¯" + " " * 51 + "â”‚\n")
				
			f.write("â””" + "â”€" * 78 + "â”˜\n\n")
		
def extract_json(text, **kwargs):
	def _extract_json(text):
		# Use regular expressions to find all content within curly braces
		orig_text = text

		text = re.sub(r'"([^"\\]*(\\.[^"\\]*)*)"', lambda m: m.group().replace('\n', r'\\n'), text) 
		
		#json_objects = re.findall(r'(\{[^{}]*\}|\[[^\[\]]*\])', text, re.DOTALL)

		def parse_json_safely(text):
			try:
				result = json.loads(text)
				return result
			except json.JSONDecodeError:
				results = []
				start = 0
				while start < len(text):
					try:
						obj, end = json.JSONDecoder().raw_decode(text[start:])
						results.append(obj)
						start += end
					except json.JSONDecodeError:
						start += 1
				
				if results:
					longest_json = max(results, key=lambda x: len(json.dumps(x)))
					return longest_json
				else:
					return None
		
		extracted_json = parse_json_safely(text)
		
		if extracted_json:
			return extracted_json
		else:
			logger.error('Error parsing response: ', orig_text)
			return None
	
	res = _extract_json(text)

	return res
