import json
import requests
import time
from datetime import datetime
import os
import pandas as pd
import pdb
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from utils import get_response, save_result, save_result_txt, extract_json, ensure_question_format
import random 
from utils import set_cache_path, init_writer, close_writer, load_file
import argparse

def parse_args():
	# åˆ›å»ºè§£æå™¨
	parser = argparse.ArgumentParser(description="evaluate completeness of profile")
	# æ·»åŠ å‚æ•°
	parser.add_argument("--model", type=str, default="gpt-4o-mini")
	parser.add_argument("--entity_path", type=str, help="è§’è‰²ä¿¡æ¯è·¯å¾„")
	parser.add_argument("--source", type=str, default="fandom", help="fandom | DRinfo")
	parser.add_argument("--entity_key", type=str, default=None, help="è¾“å…¥å­—æ®µ")
	parser.add_argument("--output_path", type=str, default="./knowledges/knowledge.json", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
	parser.add_argument("--pre_result", type=str, default=None, help="å·²æœ‰resultsè·¯å¾„")
     
	# è§£æå‚æ•°
	args = parser.parse_args()
	return args

args = parse_args()
print(args)
# exit()

# é…ç½®æ–¹æ³•é€‰æ‹©
extract_model = args.model
# language = 'en'  # é€‰æ‹© 'zh' æˆ– 'en'
entity_file = args.entity_path

pre_results_file = args.pre_result

# timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
output_file = args.output_path

parallel = True
max_workers = 5

set_cache_path('.cache-acg.pkl') # '.cache-' + output_file.replace('.json', '.pkl'))

progress_count = 0
progress_lock = threading.Lock()
total_entities = 0
save_interval = 100  # æ¯100ä¸ªå®ä½“ä¿å­˜ä¸€æ¬¡

PROMPTS = '''I have a JSON array that fails to parse with json.loads() due to invalid escape characters such as \'.
Please correct the JSON so that it becomes valid and can be successfully parsed.
You should:

- Fix all invalid escape sequences.
- Ensure every string is properly quoted.
- Return the corrected JSON only, without extra commentary.

Here is the JSON that needs to be corrected:
'''

def to_my_entity_key(entity_info):
	return entity_info[0]

def save_progress(results, filename=None):
	"""ç»Ÿä¸€çš„ä¿å­˜è¿›åº¦å‡½æ•°"""
	if filename is None:
		filename = output_file
	
	os.makedirs(os.path.dirname(filename), exist_ok=True)
	with open(filename, 'w', encoding='utf-8') as f:
		json.dump(results, f, ensure_ascii=False, indent=2)


def process_entity(entity_info):
    """å¤„ç†å•ä¸ªå®ä½“çš„å‡½æ•°ï¼Œç”¨äºå¹¶å‘æ‰§è¡Œ"""
    global progress_count, total_entities, invalid_cnt

    entity_name, entity_info = entity_info

    with progress_lock:
        progress_count += 1
        current = progress_count

    print(f"[{current}/{total_entities}] å¼€å§‹å¤„ç†å®ä½“: {entity_name}")

    # ä¿å­˜å®Œæ•´çš„å®ä½“ä¿¡æ¯
    result = {
        'entity': entity_name
    } 

    messages = []

    if not isinstance(entity_info['response'], str):
        result['response'] = entity_info['response']
        return result
    
    prompt = PROMPTS + entity_info['response']
    
    # if pre_results.get(entity_name, None):
    #     if pre_results[entity_name].get('response', None):
    #         resp = pre_results[entity_name]['response']
    #         if isinstance(resp, dict):
    #             result['response'] = resp
    #             print('already extracted')
    #             return result

    messages.append({'role': 'user', 'content': prompt})
    knowledge = get_response(model=extract_model, messages=messages)
    try:
        knowledge = knowledge.strip('```').strip('json')
        result['response'] = json.loads(knowledge)
    except:
        print('cannot parse to json format')
        result['response'] = knowledge
        invalid_cnt += 1

    # 	# ç¦»çº¿åˆ¤å®šç­”æ¡ˆå”¯ä¸€æ€§ TODO
    return result

try:
    pre_results:dict = load_file(pre_results_file)
    print(pre_results_file, 'OK')
except:
    pre_results = dict()
    print('cannot open pre_results_file')

def main():
    global total_entities

    # è¯»å–å¤šä¸ªå®ä½“æ–‡ä»¶
    entities_data = []
    # æŒ‰ä¼˜å…ˆçº§é¡ºåºè¯»å–å„ä¸ªæ–‡ä»¶
    with open(entity_file, 'r', encoding='utf-8') as f:
        df = json.load(f)
    print(f"æˆåŠŸè¯»å– {entity_file}ï¼Œå…± {len(df)} æ¡è®°å½•")
		
    # è¯»å–å®ä½“ä¿¡æ¯ï¼Œæ·»åŠ åˆ°åˆ—è¡¨ä¸­ï¼Œæ’é™¤å·²æœ‰å®ä½“
    entities_data = list(df.items())

    # å±•ç¤ºpopularityåˆ†å¸ƒ
    print(f"æ€»å…±è¯»å–äº† {len(entities_data)} ä¸ªå®ä½“")

    total_entities = len(entities_data)

    # æ ¹æ®æ–¹æ³•è°ƒæ•´å¹¶å‘æ•°
    # åˆå§‹åŒ–ç»“æœå­—å…¸ï¼ŒåŒ…å«å·²æœ‰ç»“æœå’Œæ–°å®ä½“
    results = {}

    if parallel:
        completed_count = 0
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_entity = {executor.submit(process_entity, entity_info): to_my_entity_key(entity_info) for entity_info in entities_data}
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_entity):
                entity_key = future_to_entity[future]
                result = future.result()
                results[entity_key] = result
                completed_count += 1
                
                # æ¯100ä¸ªå®ä½“ä¿å­˜ä¸€æ¬¡
                if completed_count % save_interval == 0:
                    print(f"ğŸ’¾ å·²å®Œæˆ {completed_count} ä¸ªå®ä½“ï¼Œä¿å­˜ä¸­é—´ç»“æœ...")
                    save_progress(results)
                    print(f"ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜: results/{output_file}")

    else:
        for i, entity_info in enumerate(entities_data, 1):
            result = process_entity(entity_info)
            results[to_my_entity_key(entity_info)] = result
            
            # æ¯100ä¸ªå®ä½“ä¿å­˜ä¸€æ¬¡
            if i % save_interval == 0:
                print(f"ğŸ’¾ å·²å®Œæˆ {i} ä¸ªå®ä½“ï¼Œä¿å­˜ä¸­é—´ç»“æœ...")
                save_progress(results)
                print(f"ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜: results/{output_file}")

    # ç»Ÿè®¡ç»“æœï¼šå·²æœ‰æ•°æ® + æ–°å®Œæˆçš„æ•°æ®
    total_completed = len([result for result in results.values() if result is not None])
    new_completed = len([result for entity_info in entities_data for result in [results[to_my_entity_key(entity_info)]] if result is not None])
    new_failed = len(entities_data) - new_completed

    print(f"ğŸ“Š ç»Ÿè®¡ç»“æœ:")
    # print(f"  å·²æœ‰å®ä½“: {len(existing_entitiy_keys)}")
    print(f"  æ–°å¤„ç†å®ä½“: {len(entities_data)}")
    print(f"  æ–°æˆåŠŸ: {new_completed}, æ–°å¤±è´¥: {new_failed}")
    print(f"  æ€»è®¡æˆåŠŸ: {total_completed}, æ€»è®¡å®ä½“: {len(results)}")

    # ä¿å­˜æ±‡æ€»ç»“æœ
    save_progress(results, output_file)

    # ä¿å­˜TXTæ ¼å¼çš„ç®€åŒ–ç»“æœ
    # save_result_txt(f'results/{output_file}_simple.txt', results)

    print(f"ğŸ“„ JSONæ ¼å¼: results/{output_file}.json")
    # print(f"ğŸ“„ TXTæ ¼å¼: results/{output_file}_simple.txt")

invalid_cnt = 0

if __name__ == "__main__":
    main()
    print(invalid_cnt)
    