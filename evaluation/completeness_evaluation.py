import json
import requests
import time
from datetime import datetime
import os
import pandas as pd
import pdb
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from utils import get_response, save_result, save_result_txt, extract_json, ensure_question_format
import random 
from utils import load_file, set_cache_path, init_writer, close_writer
import argparse

def parse_args():
	# åˆ›å»ºè§£æå™¨
	parser = argparse.ArgumentParser(description="evaluate completeness of profile")
	# æ·»åŠ å‚æ•°
	parser.add_argument("--model", type=str, default="gpt-4o-mini")

	parser.add_argument("--knowledge_path", type=str, required=True, help="knowledge path")

	parser.add_argument("--text_type", type=str, default="profile", help="the path of profile searched by LLM")
	parser.add_argument("--text_path", type=str, required=True, help="the path of profile searched by LLM")
	parser.add_argument("--entity_key", type=str, default=None, help="è¾“å…¥å¯¹æ¯”çš„å­—æ®µ")
	parser.add_argument("--output_path", type=str, required=True, help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
	parser.add_argument("--pre_result", type=str, default=None, help="å·²æœ‰resultsè·¯å¾„")

	# è§£æå‚æ•°
	args = parser.parse_args()
	return args

args = parse_args()
print(args)

# é…ç½®æ–¹æ³•é€‰æ‹©
compare_model = args.model
# language = 'zh'  # é€‰æ‹© 'zh' æˆ– 'en'
# profile_method = 'doubao_search'

# profile_file = "D:/å¤æ—¦å¤§å­¦/ç ”ç©¶ç”Ÿ/RPLA/DRCharater-main/gen/results/gemini_search/gemini_acg_characters_old.json"
profile_file = args.text_path
gt_konwledge_file = args.knowledge_path

pre_results_file = args.pre_result

# timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
output_file = args.output_path

parallel = True
max_workers = 10

set_cache_path('.cache-acg.pkl') # '.cache-' + output_file.replace('.json', '.pkl'))

progress_count = 0
progress_lock = threading.Lock()
total_entities = 0
save_interval = 10  # æ¯100ä¸ªå®ä½“ä¿å­˜ä¸€æ¬¡

def to_my_entity_key(entity_info):
	return entity_info[0]

def save_progress(results, filename=None):
	"""ç»Ÿä¸€çš„ä¿å­˜è¿›åº¦å‡½æ•°"""
	if filename is None:
		filename = output_file
	
	os.makedirs(os.path.dirname(filename), exist_ok=True)
	with open(filename, 'w', encoding='utf-8') as f:
		json.dump(results, f, ensure_ascii=False, indent=2)

def process_entity(entity_info):
	"""å¤„ç†å•ä¸ªå®ä½“çš„å‡½æ•°ï¼Œç”¨äºå¹¶å‘æ‰§è¡Œ"""
	global progress_count, total_entities, invalid_cnt

	entity_name, knowledge_list, character_text = entity_info

	with progress_lock:
		progress_count += 1
		current = progress_count

	print(f"[{current}/{total_entities}] å¼€å§‹æ¯”è¾ƒå®ä½“: {entity_name}")

	pre_result = pre_results.get(entity_name, None)
	# ä¿å­˜å®Œæ•´çš„å®ä½“ä¿¡æ¯
	result = {
		'entity': entity_name,
	} 

	## has been evaluated
	if pre_result and isinstance(pre_result.get('response', None), list):
		result['response'] = pre_result['response']
		return result

	from prompt import COMPARE_PROMPT

	messages = []

	prompt = COMPARE_PROMPT.replace('{knowledge_list}', json.dumps(knowledge_list, ensure_ascii=False, indent=2)).replace('{character_text}', character_text)
	messages.append({'role': 'user', 'content': prompt})

	response = get_response(model=compare_model, messages=messages)
	
	try:
		response = response.strip('```').strip('json')
		response = json.loads(response)
	except:
		invalid_cnt += 1
		print('cannot parse to json format')

	result['response'] = response

	if response is None:
		return result

	# 	# ç¦»çº¿åˆ¤å®šç­”æ¡ˆå”¯ä¸€æ€§ TODO
	return result

def get_input_data():
	gt = load_file(gt_konwledge_file)
	print(f"æˆåŠŸè¯»å– {gt_konwledge_file}ï¼Œå…± {len(gt)} æ¡è®°å½•")
	profile_full = load_file(profile_file)
	print(f"æˆåŠŸè¯»å– {profile_file}ï¼Œå…± {len(profile_full)} æ¡è®°å½•")
	
	entity_key = args.entity_key
	# if language == 'en':  entity_key = 'english_profile'
	# elif language == 'zh': entity_key = 'chinese_profile'

	entities_data = []
	for entity_name in gt.keys():
		if not isinstance(gt[entity_name]['response'], dict) or not gt[entity_name]['response'].get('knowledge_points', None): continue
		if entity_name in profile_full:
			knowledges = gt[entity_name]['response']['knowledge_points']
			knowledge_list = [{'id': i, 'knowledge': knowledge['knowledge']} for i, knowledge in enumerate(knowledges, start=1)]
			if entity_key is None and profile_full[entity_name]:
				character_text = json.dumps(profile_full[entity_name], ensure_ascii=False)
				entities_data.append((entity_name, knowledge_list, character_text))
			elif profile_full[entity_name].get(entity_key, None):
				character_text = profile_full[entity_name][entity_key]
				entities_data.append((entity_name, knowledge_list, character_text))

	return entities_data


def main():
	global total_entities
	
	# å±•ç¤ºpopularityåˆ†å¸ƒ
	entities_data = get_input_data()
	print(f"æ€»å…±è¯»å–äº† {len(entities_data)} ä¸ªå®ä½“")
	
	total_entities = len(entities_data)

	# init_writer(f"{search_model}_response.jsonl")
	# æ ¹æ®æ–¹æ³•è°ƒæ•´å¹¶å‘æ•°
	
	# åˆå§‹åŒ–ç»“æœå­—å…¸ï¼ŒåŒ…å«å·²æœ‰ç»“æœå’Œæ–°å®ä½“
	results = {}

	if parallel:
		completed_count = 0
		with ThreadPoolExecutor(max_workers=max_workers) as executor:
			# æäº¤æ‰€æœ‰ä»»åŠ¡
			future_to_entity = {executor.submit(process_entity, entity_info): to_my_entity_key(entity_info) for entity_info in entities_data}
			
			# å¤„ç†å®Œæˆçš„ä»»åŠ¡
			for future in as_completed(future_to_entity):
				entity_key = future_to_entity[future]
				result = future.result()
				results[entity_key] = result
				completed_count += 1
				
				# æ¯100ä¸ªå®ä½“ä¿å­˜ä¸€æ¬¡
				if completed_count % save_interval == 0:
					print(f"ğŸ’¾ å·²å®Œæˆ {completed_count} ä¸ªå®ä½“ï¼Œä¿å­˜ä¸­é—´ç»“æœ...")
					save_progress(results)
					print(f"ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜: results/{output_file}")

	else:
		for i, entity_info in enumerate(entities_data, 1):
			result = process_entity(entity_info)
			results[to_my_entity_key(entity_info)] = result
			
			# æ¯100ä¸ªå®ä½“ä¿å­˜ä¸€æ¬¡
			if i % save_interval == 0:
				print(f"ğŸ’¾ å·²å®Œæˆ {i} ä¸ªå®ä½“ï¼Œä¿å­˜ä¸­é—´ç»“æœ...")
				save_progress(results)
				print(f"ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜: results/{output_file}")

	# ç»Ÿè®¡ç»“æœï¼šå·²æœ‰æ•°æ® + æ–°å®Œæˆçš„æ•°æ®
	total_completed = len([result for result in results.values() if result is not None])
	new_completed = len([result for entity_info in entities_data for result in [results[to_my_entity_key(entity_info)]] if result is not None])
	new_failed = len(entities_data) - new_completed
	
	print(f"ğŸ“Š ç»Ÿè®¡ç»“æœ:")
	# print(f"  å·²æœ‰å®ä½“: {len(existing_entitiy_keys)}")
	print(f"  æ–°å¤„ç†å®ä½“: {len(entities_data)}")
	print(f"  æ–°æˆåŠŸ: {new_completed}, æ–°å¤±è´¥: {new_failed}")
	print(f"  æ€»è®¡æˆåŠŸ: {total_completed}, æ€»è®¡å®ä½“: {len(results)}")

	# ä¿å­˜æ±‡æ€»ç»“æœ
	save_progress(results, output_file)

	# ä¿å­˜TXTæ ¼å¼çš„ç®€åŒ–ç»“æœ
	# save_result_txt(f'results/{output_file}_simple.txt', results)

	# print(f"ğŸ“ ç»“æœä¿å­˜åœ¨ results/ ç›®å½•ä¸‹ï¼ˆä½¿ç”¨{profiling_model}æ–¹æ³•ï¼‰")
	print(f"ğŸ“„ JSONæ ¼å¼: results/{output_file}.json")
	# print(f"ğŸ“„ TXTæ ¼å¼: results/{output_file}_simple.txt")

	# close_writer()

invalid_cnt = 0

if __name__ == "__main__":
	try: 
		with open(pre_results_file, 'r', encoding='utf-8') as f:
			pre_results: dict = json.load(f)
			print(pre_results_file)
	except:
		pre_results = dict()
		print('no previous results or fail to parse')
	main()
	print('invalid: ', invalid_cnt)